import streamlit as st
import os
from openai import AzureOpenAI
from dotenv import load_dotenv

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ í•„ìš”)
load_dotenv()

search_endpoint = os.getenv("SEARCH_ENDPOINT")
search_key = os.getenv("SEARCH_KEY")
search_index = os.getenv("SEARCH_INDEX_NAME")

semantic_configuration = "healthy-eating-habits-data1-semantic-configuration"
query_type = "vector_semantic_hybrid"

st.title("ğŸ¤– ë‚˜ì˜ ì²« AI ì±—ë´‡")


# 2. Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
client = AzureOpenAI(
    api_key=os.getenv("AZURE_OAI_KEY"),
    azure_endpoint=os.getenv("AZURE_OAI_ENDPOINT"),
    api_version="2025-01-01-preview",  # ìµœì‹  ë²„ì „
)

# 3. ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# 4. í™”ë©´ì— ê¸°ì¡´ ëŒ€í™” ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# 5. ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
if prompt := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"):
    # (1) ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ì €ì¥
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # (2) AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # ë°°í¬ëª…
            messages=[
                {"role": m["role"], "content": m["content"]}
                for m in st.session_state.messages
            ],
            max_tokens=6553,
            temperature=0.7,
            top_p=0.95,
            frequency_penalty=0,
            presence_penalty=0,
            stop=None,
            stream=False,
            extra_body={
            "data_sources": [{
                "type": "azure_search",
                "parameters": {
                    "endpoint": f"{search_endpoint}",
                    "index_name": search_index,
                    "semantic_configuration": semantic_configuration,
                    "query_type": query_type,
                    "fields_mapping": {},
                    "in_scope": True,
                    "filter": None,
                    "strictness": 3,
                    "top_n_documents": 5,
                    "authentication": {
                    "type": "api_key",
                    "key": f"{search_key}"
                    },
                    "embedding_dependency": {
                    "type": "deployment_name",
                    "deployment_name": "text-embedding-ada-002"
                    }
                }
                }]
            }
        )

        # AI ë‹µë³€ í™”ë©´ì— í‘œì‹œ
        assistant_reply = response.choices[0].message.content
        st.markdown(assistant_reply)

    # (3) AI ì‘ë‹µ ì„¸ì…˜ì— ì €ì¥
    st.session_state.messages.append({"role": "assistant", "content": assistant_reply})
